{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launches Mongo Client\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "biased_news = client.events.biased_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create news bias events\n",
    "db = client.events\n",
    "biased_news = db.biased_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicate articles\n",
    "list(biased_news.aggregate([{'$group' : {'_id': '$title', 'count': {'$sum': 1}}},\n",
    "    {'$match': {'count': {'$gte': 2}}},\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cursor\n",
    "cursor = biased_news.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads from Mongo\n",
    "true_df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete columns and drop null values\n",
    "del true_df['sub_title']\n",
    "del true_df['_id']\n",
    "true_df = true_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterates through dataframe and drops redundant articles\n",
    "for title in eliminate.index:\n",
    "    huh = true_df['title'] == title\n",
    "    wha = true_df[huh]\n",
    "    label = wha.index\n",
    "    true_df = true_df.drop(labels=label[1:],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of stop words\n",
    "stopwords = stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Removes stop words and changes word to stem words'''\n",
    "    cleaned_text = []\n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        for word in post.split():\n",
    "            low_word = word.lower()#stemmer.stem(word.lower())\n",
    "            if low_word not in stopwords:\n",
    "                cleaned_words.append(low_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the text\n",
    "cleaned_text = clean_text(true_df.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nouns(text_list):\n",
    "    #Drops the nouns\n",
    "    no_nouns = []\n",
    "    cleaned_text = []\n",
    "    phrases = []\n",
    "    for x in text_list:\n",
    "        words = pos_tag(word_tokenize(x))\n",
    "        werdz3 = ['NNP', 'NN', 'NNP', 'NNPs', 'NNS', ',', '.', ':', '(', ')', '#', '``']\n",
    "        werdz = [s for s in words if s[-1] not in werdz3]\n",
    "        no_nouns.append(' '.join(werdz))\n",
    "    return no_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops nouns and other terms from the text\n",
    "final_round_clean = drop_nouns(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fits tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 4),  \n",
    "                                   stop_words='english', \n",
    "                                   #token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "cor_tfidf = tfidf.fit_transform(final_round_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fits LSA\n",
    "lsa = TruncatedSVD(140, algorithm = 'arpack')\n",
    "corpus_lsa = lsa.fit_transform(cor_tfidf)\n",
    "corpus_lsa = Normalizer(copy=False).fit_transform(corpus_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check explained variance\n",
    "sum(lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get terms\n",
    "terms = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check topics modeled\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    Terms_in_Comp = zip(terms,comp)\n",
    "    sorted_Terms = sorted(Terms_in_Comp, key = lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Topic %d:\" %i)\n",
    "    for term in sorted_Terms:\n",
    "        print(term[0])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates test-train split\n",
    "X = corpus_lsa\n",
    "y = true_df.source\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check against dummy classifer\n",
    "dumb = DummyClassifier(strategy = \"stratified\", random_state=0)\n",
    "dumb.fit(X_train, y_train)\n",
    "print(dumb.score(X_test, y_test))\n",
    "print(dumb.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fits a random forest classifier\n",
    "rf2 = RandomForestClassifier(n_estimators=50)#, max_depth=36)\n",
    "rf2.fit(X_train, y_train)\n",
    "print(rf2.score(X_test, y_test))\n",
    "print(rf2.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the confusion matrix\n",
    "names = ['Fox News', 'National Review', 'Breitbart', 'Info wars', 'Global Research',\n",
    "       'Activist Post', 'Reuters', 'Associate Press',\n",
    "       'Alabama Today', 'Huffington Post', 'Daily Beast', 'Mother Jones']\n",
    "plt.figure(dpi=100)\n",
    "cm = confusion_matrix(y_test, rf2.predict(X_test), labels =names)\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.colorbar();\n",
    "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ('FN', 'NR', 'B', 'IW', 'GR',\n",
    "       'ActP', 'R', 'AP',\n",
    "       'AT', 'HP', 'DB', 'MJ'))\n",
    "plt.yticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ('Fox News', 'National Review', 'Breitbart', 'Info wars', 'Global Research',\n",
    "       'Activist Post', 'Reuters', 'Associated Press',\n",
    "       'Alabama Today', 'Huffington Post', 'Daily Beast', 'Mother Jones'));\n",
    "plt.ylabel(\"True Source\")\n",
    "plt.xlabel(\"Predicted Source\");\n",
    "fmt = '.1f'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j]),#, fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the classification report\n",
    "print(classification_report(y_test, rf2.predict(X_test), target_names=names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
